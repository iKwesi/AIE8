{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47eTBHYNP4g1"
      },
      "source": [
        "# Introduction to LCEL and LangGraph: LangChain Powered RAG\n",
        "\n",
        "In the following notebook we're going to focus on learning how to navigate and build useful applications using LangChain, specifically LCEL, and how to integrate different APIs together into a coherent RAG application!\n",
        "\n",
        "We'll be building a RAG system to answer questions about how people use AI, using the \"How People Use AI\" dataset.\n",
        "\n",
        "In the notebook, you'll complete the following Tasks:\n",
        "\n",
        "- 🤝 Breakout Room #2:\n",
        "    1. LangChain and LCEL Concepts\n",
        "    2. Understanding States and Nodes\n",
        "    3. Introduction to QDrant Vector Databases\n",
        "    4. Building a Basic Graph\n",
        "\n",
        "Let's get started!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation Requirements\n",
        "\n",
        "Also, make sure Ollama is installed and running with the required models pulled (see instructions below).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: LangSmith Setup for Tracing and Monitoring\n",
        "\n",
        "LangSmith provides powerful tracing, monitoring, and debugging capabilities for LangChain applications. While not required for this notebook, setting it up will give you valuable insights into your RAG system's performance.\n",
        "\n",
        "### Getting LangSmith Credentials\n",
        "\n",
        "1. **Sign up for LangSmith**: Visit [smith.langchain.com](https://smith.langchain.com) and create a free account\n",
        "2. **Get your API Key**: \n",
        "   - Go to Settings → API Keys\n",
        "   - Create a new API key and copy it\n",
        "3. **Set your environment variables** (choose one method below):\n",
        "\n",
        "**Option A: Set environment variables in your terminal before starting Jupyter:**\n",
        "```bash\n",
        "export LANGCHAIN_TRACING_V2=true\n",
        "export LANGCHAIN_API_KEY=\"your-api-key-here\"\n",
        "export LANGCHAIN_PROJECT=\"RAG-Assignment\"\n",
        "```\n",
        "\n",
        "**Option B: Set them in the notebook (run the cell below):**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangSmith tracing enabled: true\n",
            "Project name: RAG-Assignment\n"
          ]
        }
      ],
      "source": [
        "# Optional: Set up LangSmith tracing\n",
        "# Uncomment and fill in your credentials if you want to use LangSmith\n",
        "\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "# Uncomment the lines below to enable LangSmith tracing\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG-Assignment\"\n",
        "\n",
        "# Verify setup (uncomment to check)\n",
        "print(\"LangSmith tracing enabled:\", os.getenv(\"LANGCHAIN_TRACING_V2\", \"false\"))\n",
        "print(\"Project name:\", os.getenv(\"LANGCHAIN_PROJECT\", \"Not set\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What LangSmith Provides\n",
        "\n",
        "Once set up, LangSmith will automatically trace your LangChain operations and provide:\n",
        "\n",
        "- **Execution traces**: See exactly how your RAG pipeline processes each query\n",
        "- **Performance metrics**: Monitor latency, token usage, and costs\n",
        "- **Debugging tools**: Inspect intermediate outputs at each step\n",
        "- **Error tracking**: Identify and debug issues in your chains\n",
        "- **Dataset management**: Collect and organize your queries and responses\n",
        "\n",
        "You can view all traces and analytics in your LangSmith dashboard at [smith.langchain.com](https://smith.langchain.com).\n",
        "\n",
        "> **Note**: LangSmith is completely optional for this assignment. The notebook will work perfectly fine without it, but it's a valuable tool for production applications.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ayVXHXHRE_t"
      },
      "source": [
        "# 🤝 Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl6wTp9C5qbY"
      },
      "source": [
        "## Set Up Ollama\n",
        "\n",
        "We'll be using Ollama to run local LLM models. Make sure you have Ollama installed and running:\n",
        "\n",
        "1. Install Ollama from https://ollama.ai (`curl https://ollama.ai/install.sh | sh`)\n",
        "2. Make sure the output of `ollama -v` reads `0.11.10` or greater.\n",
        "2. Pull the models we'll use:\n",
        "   ```bash\n",
        "   ollama pull gpt-oss:20b # For the chat model\n",
        "   ollama pull embeddinggemma:latest  # For embeddings\n",
        "   ```\n",
        "3. Ensure Ollama is running (it should start automatically after installation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTsujAkpRWpJ"
      },
      "source": [
        "### A Note On Runnables\n",
        "\n",
        "# Understanding LangChain Runnables and LCEL\n",
        "\n",
        "In LangChain, a Runnable is like a LEGO brick in your AI application - it's a standardized component that can be easily connected with other components. The real power of Runnables comes from their ability to be combined in flexible ways using LCEL (LangChain Expression Language).\n",
        "\n",
        "## Key Features of Runnables\n",
        "\n",
        "### 1. Universal Interface\n",
        "Every Runnable in LangChain follows the same pattern:\n",
        "- Takes an input\n",
        "- Performs some operation\n",
        "- Returns an output\n",
        "\n",
        "This consistency means you can treat different components (like models, retrievers, or parsers) in the same way.\n",
        "\n",
        "### 2. Built-in Parallelization\n",
        "Runnables come with methods for handling multiple inputs efficiently:\n",
        "```python\n",
        "# Process inputs in parallel, maintain order\n",
        "results = chain.batch([input1, input2, input3])\n",
        "\n",
        "# Process inputs as they complete\n",
        "for result in chain.batch_as_completed([input1, input2, input3]):\n",
        "    print(result)\n",
        "```\n",
        "\n",
        "### 3. Streaming Support\n",
        "Perfect for responsive applications:\n",
        "```python\n",
        "# Stream outputs as they're generated\n",
        "for chunk in chain.stream({\"query\": \"Tell me a story\"}):\n",
        "    print(chunk, end=\"\", flush=True)\n",
        "```\n",
        "\n",
        "### 4. Easy Composition\n",
        "The `|` operator makes building pipelines intuitive:\n",
        "```python\n",
        "# Create a basic RAG chain\n",
        "rag_chain = retriever | prompt | model | output_parser\n",
        "```\n",
        "\n",
        "## Common Types of Runnables\n",
        "\n",
        "- **Language Models**: Like our `ChatOllama` instance (running locally with Ollama)\n",
        "- **Prompt Templates**: Format inputs consistently\n",
        "- **Retrievers**: Get relevant context from a vector store\n",
        "- **Output Parsers**: Structure model outputs\n",
        "- **LangGraph Nodes**: Individual components in our graph\n",
        "\n",
        "Think of Runnables as the building blocks of your LLM application. Just like how you can combine LEGO bricks in countless ways, you can mix and match Runnables to create increasingly sophisticated applications!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaVlJiilDzwM"
      },
      "source": [
        "## LangGraph Based RAG\n",
        "\n",
        "Now that we have a reasonable grasp of LCEL and the idea of Runnables - let's see how we can use LangGraph to build the same system!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I77-NKo1EowG"
      },
      "source": [
        "### Primer: What is LangGraph?\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "#### Why Cycles?\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effectively allowing us to recreate application flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "#### Why LangGraph?\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!\n",
        "\n",
        "> NOTE: We're going to focus on building a simple DAG for today's assignment as an introduction to LangGraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfLCnMXNE_Qc"
      },
      "source": [
        "### Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "> coordinated multi-actor and stateful applications\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "However, in our example here, we're focusing on a simpler `State` object:\n",
        "\n",
        "```python\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    context: list[Document]\n",
        "    response: str\n",
        "```\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. **We initialize our state object**:\n",
        "   ```python\n",
        "   {\n",
        "       \"question\": \"\",\n",
        "       \"context\": [],\n",
        "       \"response\": \"\"\n",
        "   }\n",
        "   ```\n",
        "\n",
        "2. **Our user submits a query to our application.**  \n",
        "   We store the user's question in `state[\"question\"]`. Now we have:\n",
        "   ```python\n",
        "   {\n",
        "       \"question\": \"How tall is the Eiffel Tower?\",\n",
        "       \"context\": [],\n",
        "       \"response\": \"\"\n",
        "   }\n",
        "   ```\n",
        "\n",
        "3. **We pass our state object to an Agent node** which is able to read the current state. It will use the value of `state[\"question\"]` as input and might retrieve some context documents related to the question. It then generates a response which it stores in `state[\"response\"]`. For example:\n",
        "   ```python\n",
        "   {\n",
        "       \"question\": \"How tall is the Eiffel Tower?\",\n",
        "       \"context\": [Document(page_content=\"...some data...\")],\n",
        "       \"response\": \"The Eiffel Tower is about 324 meters tall...\"\n",
        "   }\n",
        "   ```\n",
        "\n",
        "That's it! The important part is that we have a consistent object (`State`) that's passed around, holding the crucial information as we go from one node to the next. This ensures our application has a single source of truth about what has happened so far and what is happening now.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kxczzsfVFNWT"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import START, StateGraph\n",
        "from typing_extensions import TypedDict\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "class State(TypedDict):\n",
        "  question: str\n",
        "  context: list[Document]\n",
        "  response: str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l6xFY0_HoXG"
      },
      "source": [
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL Runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keL9O1drInw1"
      },
      "source": [
        "### Building Nodes\n",
        "\n",
        "We're going to need two nodes:\n",
        "\n",
        "A node for retrieval, and a node for generation.\n",
        "\n",
        "Let's start with our `retrieve` node!\n",
        "\n",
        "Notice how we do not need to update the state object in the node, but can instead return a modification directly to our state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Building a Retriever with LangChain\n",
        "\n",
        "In order to build our `retrieve` node, we'll first need to build a retriever!\n",
        "\n",
        "This will involve the following steps: \n",
        "\n",
        "1. Ingesting Data\n",
        "2. Chunking the Data\n",
        "3. Vectorizing the Data and Storing it in a Vector Database\n",
        "4. Converting it to a Retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Retreiver Step 1: Ingesting Data\n",
        "\n",
        "In today's lesson, we're going to be building a RAG system to answer questions about how people use AI - and we will pull information into our index (vectorized chunks stored in our vector store) through LangChain's [`PyMuPDFLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyMuPDFLoader.html)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> NOTE: We'll be using an async loader during our document ingesting - but our Jupyter Kernel is already running in an asyc loop! This means we'll want the ability to *nest* async loops. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we're good to load our documents through the [`PyMuPDFLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyMuPDFLoader.html)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "directory_loader = DirectoryLoader(\"data\", glob=\"**/*.pdf\", loader_cls=PyMuPDFLoader)\n",
        "\n",
        "ai_usage_knowledge_resources = directory_loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'NBER WORKING PAPER SERIES\\nHOW PEOPLE USE CHATGPT\\nAaron Chatterji\\nThomas Cunningham\\nDavid J. Deming\\nZoe Hitzig\\nChristopher Ong\\nCarl Yan Shan\\nKevin Wadman\\nWorking Paper 34255\\nhttp://www.nber.org/papers/w34255\\nNATIONAL BUREAU OF ECONOMIC RESEARCH\\n1050 Massachusetts Avenue\\nCambridge, MA 02138\\nSeptember 2025\\nWe acknowledge help and comments from Joshua Achiam, Hemanth Asirvatham, Ryan \\nBeiermeister, Rachel Brown, Cassandra Duchan Solis, Jason Kwon, Elliott Mokski, Kevin Rao, \\nHarrison Satcher, Gawesha Weeratunga, Hannah Wong, and Analytics & Insights team. We \\nespecially thank Tyna Eloundou and Pamela Mishkin who in several ways laid the foundation for \\nthis work. This study was approved by Harvard IRB (IRB25-0983). A repository containing all \\ncode run to produce the analyses in this paper is available on request. The views expressed herein \\nare those of the authors and do not necessarily reflect the views of the National Bureau of \\nEconomic Research.\\nAt least one co-author has disclosed a'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ai_usage_knowledge_resources[0].page_content[:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### TextSplitting aka Chunking\n",
        "\n",
        "We'll use the `RecursiveCharacterTextSplitter` to create our toy example.\n",
        "\n",
        "It will split based on the following rules:\n",
        "\n",
        "- Each chunk has a maximum size of 1000 tokens\n",
        "- It will try and split first on the `\\n\\n` character, then on the `\\n`, then on the `<SPACE>` character, and finally it will split on individual tokens.\n",
        "\n",
        "Let's implement it and see the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def tiktoken_len(text):\n",
        "    # Using cl100k_base encoding which is a good general-purpose tokenizer\n",
        "    # This works well for estimating token counts even with Ollama models\n",
        "    tokens = tiktoken.get_encoding(\"cl100k_base\").encode(\n",
        "        text,\n",
        "    )\n",
        "    return len(tokens)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 750,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = tiktoken_len,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "ai_usage_knowledge_chunks = text_splitter.split_documents(ai_usage_knowledge_resources)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 🏗️ Activity #1:\n",
        "\n",
        "While there's nothing specifically wrong with the chunking method used above - it is a naive approach that is not sensitive to specific data formats.\n",
        "\n",
        "Brainstorm some ideas that would split large single documents into smaller documents.\n",
        "\n",
        "✅ Answer:\n",
        "\n",
        "#### 1. Semantic-Aware Chunking  \n",
        "Instead of splitting text by arbitrary character counts, this method leverages linguistic signals to locate natural breakpoints. The goal is to preserve the flow of ideas by aligning chunks with topic shifts, paragraphs, or section breaks.  \n",
        "\n",
        "**Steps:**  \n",
        "- Analyze document hierarchy (sections, sub-sections, paragraphs)  \n",
        "- Use sentence segmentation to determine logical cut points  \n",
        "- Apply overlapping windows to minimize context loss  \n",
        "- Ensure chunks remain within token limits while retaining coherence  \n",
        "\n",
        "---\n",
        "\n",
        "#### 2. Metadata-Guided Chunking  \n",
        "When documents include metadata (e.g., titles, abstracts, timestamps), these labels can drive segmentation. This ensures chunks align with the logical structure of the source and retain meaningful context.  \n",
        "\n",
        "**Steps:**  \n",
        "- Extract metadata fields from structured files (JSON, XML, PDFs)  \n",
        "- Segment content based on section labels (abstract, methods, references)  \n",
        "- Apply additional splitting within long sections if needed  \n",
        "- Store metadata tags with each chunk for context-aware retrieval  \n",
        "\n",
        "---\n",
        "\n",
        "#### 3. Conversation-Aware Chunking  \n",
        "Designed for transcripts or chats, this approach respects conversational flow. It prevents splitting mid-turn and ensures dialogue is kept coherent with speaker attribution intact.  \n",
        "\n",
        "**Steps:**  \n",
        "- Break dialogue into speaker–utterance units  \n",
        "- Group utterances until token or size limits are reached  \n",
        "- Preserve speaker labels in each chunk  \n",
        "- Add overlapping turns so conversational context is maintained  \n",
        "\n",
        "---\n",
        "\n",
        "#### 4. Token-Level Sliding Window Chunking  \n",
        "This method aligns chunking directly with the tokenization scheme of a target LLM. It optimizes chunk sizes for model capacity and ensures transitions between segments remain smooth.  \n",
        "\n",
        "**Steps:**  \n",
        "- Tokenize text using a model-specific tokenizer (e.g., `tiktoken`)  \n",
        "- Slide a token window across text with controlled overlap  \n",
        "- Prefer boundaries at sentence or clause endings where possible  \n",
        "- Record positional metadata to enable reconstruction  \n",
        "\n",
        "---\n",
        "\n",
        "#### 5. Embedding-Driven Clustering Chunking  \n",
        "This approach forms chunks by grouping semantically similar text units based on embeddings, rather than relying only on sequence. It ensures chunks reflect thematic coherence.  \n",
        "\n",
        "**Steps:**  \n",
        "- Generate embeddings for sentences or paragraphs  \n",
        "- Cluster embeddings using methods like K-means or HDBSCAN  \n",
        "- Combine clustered units into size-constrained blocks  \n",
        "- Attach cluster identifiers to chunks to preserve semantic grouping  \n",
        "\n",
        "---\n",
        "\n",
        "#### 6. Graph-Structured Dependency Chunking  \n",
        "This advanced technique models a document as a graph of entities and their relationships. Chunks are derived from subgraphs, preserving dependencies and logical connections.  \n",
        "\n",
        "**Steps:**  \n",
        "- Extract entities and relationships via NER or dependency parsing  \n",
        "- Construct a graph with entities as nodes and relations as edges  \n",
        "- Partition the graph into cohesive subgraphs using clustering algorithms  \n",
        "- Create chunks aligned with these subgraphs to maintain relationa\n",
        "\n",
        "These approaches would create more coherent and contextually meaningful chunks compared to the simple character-based splitting used in the notebook, leading to better retrieval performance in the RAG system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Embeddings and Dense Vector Search\n",
        "\n",
        "Now that we have our individual chunks, we need a system to correctly select the relevant pieces of information to answer our query.\n",
        "\n",
        "This sounds like a perfect job for embeddings!\n",
        "\n",
        "We'll be using Ollama's `embeddinggemma` model as our embedding model today! This is a powerful open-source embedding model that runs locally.\n",
        "\n",
        "Let's load it up through LangChain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_ollama import OllamaEmbeddings\n",
        " \n",
        "# Using embeddinggemma which is a powerful open-source embedding model\n",
        "embedding_model = OllamaEmbeddings(model=\"embeddinggemma:latest\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ❓ Question #1:\n",
        "\n",
        "What is the embedding dimension, given that we're using `embeddinggemma`?\n",
        "\n",
        "You will need to fill the next cell out correctly with your embedding dimension for the rest of the notebook to run.\n",
        "\n",
        "✅ Answer:\n",
        "Based on the documentation and also running `ollam show embeddinggemma:latest`, the answer is 768"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedding_dim = 768# YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Using A Vector Database - Intoduction to Qdrant\n",
        "\n",
        "Up to this point, we've been using a dictionary to hold our embeddings - typically, we'll want to use a more robust strategy.\n",
        "\n",
        "In this bootcamp - we'll be focusing on leveraging [Qdrant's vector database](https://qdrant.tech/qdrant-vector-database/).\n",
        "\n",
        "Let's take a look at how we set-up Qdrant!\n",
        "\n",
        "> NOTE: We'll be spending a lot of time learning about Qdrant throughout the remainder of our time together - but for an initial primer, please check out [this resource](https://qdrant.tech/articles/what-is-a-vector-database/)\n",
        "\n",
        "We are going to be using an \"in-memory\" Qdrant client, which means that our vectors will be held in our system's memory (RAM) - this is useful for prototyping and developement at smaller scales - but would need to be modified when moving to production. Luckily for us, this modification is trivial!\n",
        "\n",
        "> NOTE: While LangChain uses the terminology \"VectorStore\" (also known as a Vector Library), Qdrant is a \"Vector Database\" - more info. on that [here.](https://weaviate.io/blog/vector-library-vs-vector-database)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_qdrant import QdrantVectorStore\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import Distance, VectorParams\n",
        "\n",
        "client = QdrantClient(\":memory:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we need to create a collection - a collection is a specific...collection of vectors within the Qdrant client.\n",
        "\n",
        "These are useful as they allow us to create multiple different \"warehouses\" in a single client, which can be leveraged for personalization and more!\n",
        "\n",
        "Also notice that we define what our vector shapes are (embedding dim) as well as our desired distance metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.create_collection(\n",
        "    collection_name=\"ai_usage_knowledge_index\",\n",
        "    vectors_config=VectorParams(size=embedding_dim, distance=Distance.COSINE),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can assemble our vector database! Notice that we provide our client, our created collection, and our embedding model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "vector_store = QdrantVectorStore(\n",
        "    client=client,\n",
        "    collection_name=\"ai_usage_knowledge_index\",\n",
        "    embedding=embedding_model,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have our vector database set-up, we can add our documents into it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "_ = vector_store.add_documents(documents=ai_usage_knowledge_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Creating a Retriever\n",
        "\n",
        "Now that we have an idea of how we're getting our most relevant information - let's see how we could create a pipeline that would automatically extract the closest chunk to our query and use it as context for our prompt!\n",
        "\n",
        "This will involve a popular LangChain interace known as `as_retriever`!\n",
        "\n",
        "> NOTE: We can still specify how many documents we wish to retrieve per vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'macOS Version 15.4.1 (Build 24E263) Quartz PDFContext, AppendMode 1.1', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-12T20:05:32+00:00', 'source': 'data/howpeopleuseai.pdf', 'file_path': 'data/howpeopleuseai.pdf', 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'How People Use ChatGPT', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T10:32:36-04:00', 'trapped': '', 'modDate': \"D:20250915103236-04'00'\", 'creationDate': 'D:20250912200532Z', 'page': 34, '_id': '3da92516a8714155aa0635c749952239', '_collection_name': 'ai_usage_knowledge_index'}, page_content='Panel A. Work Related\\nPanel B1. Asking.\\nPanel B2. Doing.\\nFigure 23: (continued on next page)\\n33'),\n",
              " Document(metadata={'producer': 'macOS Version 15.4.1 (Build 24E263) Quartz PDFContext, AppendMode 1.1', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-12T20:05:32+00:00', 'source': 'data/howpeopleuseai.pdf', 'file_path': 'data/howpeopleuseai.pdf', 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'How People Use ChatGPT', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T10:32:36-04:00', 'trapped': '', 'modDate': \"D:20250915103236-04'00'\", 'creationDate': 'D:20250912200532Z', 'page': 30, '_id': 'bd25188db4e04a0289598040af8eb88c', '_collection_name': 'ai_usage_knowledge_index'}, page_content='Panel A. Work Related\\nPanel B1. Asking.\\nPanel B2. Doing.\\nPanel B3. Expressing.\\nFigure 22: (continued on next page)\\n29'),\n",
              " Document(metadata={'producer': 'macOS Version 15.4.1 (Build 24E263) Quartz PDFContext, AppendMode 1.1', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-12T20:05:32+00:00', 'source': 'data/howpeopleuseai.pdf', 'file_path': 'data/howpeopleuseai.pdf', 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'How People Use ChatGPT', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T10:32:36-04:00', 'trapped': '', 'modDate': \"D:20250915103236-04'00'\", 'creationDate': 'D:20250912200532Z', 'page': 53, '_id': 'fa78e4702e574d8485ad741d5025c849', '_collection_name': 'ai_usage_knowledge_index'}, page_content=\"E.g. 'User is rewriting email to neighbors about\\nplumbing to be more friendly,'\\nor 'User is complaining about grandmother'\\nor 'User is asking for help fixing python databricks error.'\\n31The IWA classifications were carried out by two annotators, while all other classifications had three.\\n52\"),\n",
              " Document(metadata={'producer': 'macOS Version 15.4.1 (Build 24E263) Quartz PDFContext, AppendMode 1.1', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-12T20:05:32+00:00', 'source': 'data/howpeopleuseai.pdf', 'file_path': 'data/howpeopleuseai.pdf', 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'How People Use ChatGPT', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T10:32:36-04:00', 'trapped': '', 'modDate': \"D:20250915103236-04'00'\", 'creationDate': 'D:20250912200532Z', 'page': 37, '_id': '3a2ffc8b6c4b42aea49aa7485cbb9c6d', '_collection_name': 'ai_usage_knowledge_index'}, page_content='in knowledge-intensive jobs where productivity is increasing in the quality of decision-making.\\n36'),\n",
              " Document(metadata={'producer': 'macOS Version 15.4.1 (Build 24E263) Quartz PDFContext, AppendMode 1.1', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-12T20:05:32+00:00', 'source': 'data/howpeopleuseai.pdf', 'file_path': 'data/howpeopleuseai.pdf', 'total_pages': 64, 'format': 'PDF 1.6', 'title': 'How People Use ChatGPT', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T10:32:36-04:00', 'trapped': '', 'modDate': \"D:20250915103236-04'00'\", 'creationDate': 'D:20250912200532Z', 'page': 32, '_id': 'fad02b6b3ec34ce88e38af9f231da99f', '_collection_name': 'ai_usage_knowledge_index'}, page_content='27As discussed in Section: Data and Privacy, our dataset only includes users on ChatGPT Consumer plans. Corporate\\nusers may also use ChatGPT Business (formerly known as Teams) or ChatGPT Enterprise.\\n28Very few work-related messages are classified as Expressing.\\n29Appendix D contains a full report of GWA counts broken down by occupation, for both work-related ChatGPT\\n31')]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever.invoke(\"How do people use AI in their daily work?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Creating the Node\n",
        "\n",
        "We're finally ready to create our node!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "05qhncktIwK_"
      },
      "outputs": [],
      "source": [
        "def retrieve(state: State) -> State:\n",
        "  retrieved_docs = retriever.invoke(state[\"question\"])\n",
        "  return {\"context\" : retrieved_docs}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate Node\n",
        "\n",
        "Next, let's create our `generate` node - which will leverage LangChain and something called an \"LCEL Chain\" which you can read more about [here](https://python.langchain.com/docs/concepts/lcel/)!\n",
        "\n",
        "We'll want to create a chain that does the following: \n",
        "\n",
        "1. Formats our inputs into a chat template suitable for RAG\n",
        "2. Takes that chat template and sends it to an LLM\n",
        "3. Parses that output into `str` format\n",
        "\n",
        "Let's get chaining!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Chain Components: RAG Chat Template\n",
        "\n",
        "We'll create a chat template that takes in some query and formats it as a RAG prompt using LangChain's prompt template!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "HUMAN_TEMPLATE = \"\"\"\n",
        "#CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUERY:\n",
        "{query}\n",
        "\n",
        "Use the provide context to answer the provided user query. Only use the provided context to answer the query. If you do not know the answer, or it's not contained in the provided context response with \"I don't know\"\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", HUMAN_TEMPLATE)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n#CONTEXT:\\nOUR CONTEXT HERE\\n\\nQUERY:\\nOUR QUERY HERE\\n\\nUse the provide context to answer the provided user query. Only use the provided context to answer the query. If you do not know the answer, or it\\'s not contained in the provided context response with \"I don\\'t know\"\\n'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_prompt.invoke({\"context\" : \"OUR CONTEXT HERE\", \"query\" : \"OUR QUERY HERE\"}).messages[0].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Chain Components: Generator\n",
        "\n",
        "We'll next set-up the generator - which will be Ollama's `gpt-oss:20b` model running locally!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "# Using gpt-oss:20b which is a powerful and efficient local model\n",
        "# ollama_chat_model = ChatOllama(model=\"gpt-oss:20b\", temperature=0.6)\n",
        "\n",
        "ollama_chat_model = ChatOllama(model=\"mistral:7b\", temperature=0.6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now call our model with a formatted prompt.\n",
        "\n",
        "Notice that we have some nested calls here - we'll see that this is made easier by LCEL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=' Paris', additional_kwargs={}, response_metadata={'model': 'mistral:7b', 'created_at': '2025-09-23T20:32:30.569313Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5212465750, 'load_duration': 4681535958, 'prompt_eval_count': 80, 'prompt_eval_duration': 497430667, 'eval_count': 2, 'eval_duration': 32138791, 'model_name': 'mistral:7b'}, id='run--0d5612a6-7df3-4390-ae46-b4e6cacfa776-0', usage_metadata={'input_tokens': 80, 'output_tokens': 2, 'total_tokens': 82})"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ollama_chat_model.invoke(chat_prompt.invoke({\"context\" : \"Paris is the capital of France\", \"query\" : \"What is the capital of France?\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Chain Components: `str` Parser\n",
        "\n",
        "Finally, let's set-up our `StrOutputParser()` which will transform our model's output into a simple `str` to be provided to the user.\n",
        "\n",
        "> NOTE: You can see us leveraging LCEL in the example below to avoid needing to do nested calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' Paris'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "generator_chain = chat_prompt | ollama_chat_model | StrOutputParser()\n",
        "\n",
        "generator_chain.invoke({\"context\" : \"Paris is the capital of France\", \"query\" : \"What is the capital of France?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `generate` Node: \n",
        "\n",
        "Now we can create our `generate` Node!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XiL2isC8JS0l"
      },
      "outputs": [],
      "source": [
        "def generate(state: State) -> State:\n",
        "  generator_chain = chat_prompt | ollama_chat_model | StrOutputParser()\n",
        "  response = generator_chain.invoke({\"query\" : state[\"question\"], \"context\" : state[\"context\"]})\n",
        "  return {\"response\" : response}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZtriMEcJxeR"
      },
      "source": [
        "Now we can start defining our graph!\n",
        "\n",
        "Think of the graph's state as a blank canvas that we can add nodes and edges to.\n",
        "\n",
        "Every graph starts with two special nodes - START and END - the act as the entry and exit point to the other nodes in the graphs.  \n",
        "\n",
        "All valid graphs must start at the START node and end at the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ia9IWM9AJ4bx"
      },
      "outputs": [],
      "source": [
        "# Start with the blank canvas\n",
        "graph_builder = StateGraph(State)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kro8bQEL2Yj"
      },
      "source": [
        "Now we can add a sequence to our \"canvas\" (graph) - this can be done by providing a list of nodes, the will automatically have edges that connect the i-th element to the i+1-th element in the list. The final element will be added to the END node unless otherwise specified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "OSfDMlXUL2kh"
      },
      "outputs": [],
      "source": [
        "graph_builder = graph_builder.add_sequence([retrieve, generate])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g79NZf5VL4en"
      },
      "source": [
        "Next, let's connect our START node to our `retrieve` node by adding an edge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "w1kTJKGNL4qA"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x12e8233b0>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_builder.add_edge(START, \"retrieve\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EiVyt8-L6_5"
      },
      "source": [
        "Finally we can compile our graph! This will do basic verification to ensure that the Runnables have the correct inputs/outputs and can be matched."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "TM4My6geL7FW"
      },
      "outputs": [],
      "source": [
        "graph = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNvoQcfCP3xI"
      },
      "source": [
        "Finally, we can visualize our graph!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+  \n",
            "| __start__ |  \n",
            "+-----------+  \n",
            "      *        \n",
            "      *        \n",
            "      *        \n",
            "+----------+   \n",
            "| retrieve |   \n",
            "+----------+   \n",
            "      *        \n",
            "      *        \n",
            "      *        \n",
            "+----------+   \n",
            "| generate |   \n",
            "+----------+   \n",
            "      *        \n",
            "      *        \n",
            "      *        \n",
            " +---------+   \n",
            " | __end__ |   \n",
            " +---------+   \n"
          ]
        }
      ],
      "source": [
        "# graph\n",
        "graph.get_graph().print_ascii()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRCjvvyP8DA"
      },
      "source": [
        "Let's take it for a spin!\n",
        "\n",
        "We invoke our graph like we do any other Runnable in LCEL!\n",
        "\n",
        "> NOTE: That's right, even a compiled graph is a Runnable!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "mSbsRLurKOKd",
        "outputId": "114185f3-4b98-4c66-96cd-65f4e4e3ef1d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              " Based on the provided context, the most common ways people use AI in their work are for asking questions, doing tasks, and expressing ideas (as seen in Panels B1, B2, and B3 respectively). For instance, a user might ask for help fixing a python databricks error or rewrite an email to neighbors about plumbing. However, the context also mentions that productivity is increasing in knowledge-intensive jobs where the quality of decision-making is improving, but it does not specify if this is done directly through AI usage or as a result of AI assistance."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Markdown, display\n",
        "response = graph.invoke({\"question\" : \"What are the most common ways people use AI in their work?\"})\n",
        "display(Markdown(response[\"response\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "a_jEmE_rKwED",
        "outputId": "c5fac807-2a24-4cf9-8cca-105def13e3d8"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              " I don't know. The provided context only discusses the use of AI in a work-related setting, specifically focusing on knowledge-intensive jobs and interaction quality. There is no mention of personal life uses of AI."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = graph.invoke({\"question\" : \"Do people use AI for their personal lives?\"})\n",
        "display(Markdown(response[\"response\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              " The provided context does not explicitly state concerns or challenges that people have when using AI. However, it does mention a study titled \"Underreporting of AI use: The role of social desirability bias\" (Ling, Yier and Alex Imas, May 2025) which might delve into such issues. Without the full content of the study, I can't provide more specific information."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = graph.invoke({\"question\" : \"What concerns or challenges do people have when using AI?\"})\n",
        "display(Markdown(response[\"response\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              " I don't know. The provided context does not contain information about Batman."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = graph.invoke({\"question\" : \"Who is Batman?\"})\n",
        "display(Markdown(response[\"response\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_LRYXvvRjOp"
      },
      "source": [
        "#### ❓ Question #2:\n",
        "LangGraph's graph-based approach lets us visualize and manage complex flows naturally. How could we extend our current implementation to handle edge cases? For example:\n",
        "- What if the retriever finds no relevant context?  \n",
        "- What if the response needs fact-checking?\n",
        "Consider how you would modify the graph to handle these scenarios.\n",
        "\n",
        "##### ✅ Answers\n",
        "2.1 #Your answer here\n",
        "\n",
        "2.2 #Your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Handling No Relevant Context Found\n",
        "\n",
        "**Problem**: When the retriever finds no relevant context, the system still proceeds to generation, potentially leading to hallucinated responses.\n",
        "\n",
        "**Solution**: Add a context validation node and conditional routing.\n",
        "\n",
        "#### Modified Graph Structure:\n",
        "```\n",
        "START → retrieve → validate_context → [conditional routing]\n",
        "                        ↓\n",
        "                   [has_context] → generate → END\n",
        "                        ↓\n",
        "                   [no_context] → fallback_response → END\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementation:\n",
        "\n",
        "```python\n",
        "from typing import Literal\n",
        "\n",
        "class EnhancedState(TypedDict):\n",
        "    question: str\n",
        "    context: list[Document]\n",
        "    response: str\n",
        "    context_quality: float  # New field for context relevance score\n",
        "    has_sufficient_context: bool  # New field for routing decision\n",
        "\n",
        "def validate_context(state: EnhancedState) -> EnhancedState:\n",
        "    \"\"\"Validate if retrieved context is sufficient and relevant.\"\"\"\n",
        "    context = state[\"context\"]\n",
        "    question = state[\"question\"]\n",
        "    \n",
        "    if not context or len(context) == 0:\n",
        "        return {\n",
        "            \"context_quality\": 0.0,\n",
        "            \"has_sufficient_context\": False\n",
        "        }\n",
        "    \n",
        "    # Calculate relevance score using embedding similarity\n",
        "    question_embedding = embedding_model.embed_query(question)\n",
        "    context_embeddings = [embedding_model.embed_query(doc.page_content) for doc in context]\n",
        "    \n",
        "    # Calculate average cosine similarity\n",
        "    from numpy import dot\n",
        "    from numpy.linalg import norm\n",
        "    \n",
        "    similarities = []\n",
        "    for ctx_emb in context_embeddings:\n",
        "        similarity = dot(question_embedding, ctx_emb) / (norm(question_embedding) * norm(ctx_emb))\n",
        "        similarities.append(similarity)\n",
        "    \n",
        "    avg_similarity = sum(similarities) / len(similarities)\n",
        "    \n",
        "    # Set threshold for sufficient context (adjustable)\n",
        "    threshold = 0.3\n",
        "    \n",
        "    return {\n",
        "        \"context_quality\": avg_similarity,\n",
        "        \"has_sufficient_context\": avg_similarity >= threshold\n",
        "    }\n",
        "\n",
        "def fallback_response(state: EnhancedState) -> EnhancedState:\n",
        "    \"\"\"Generate response when no sufficient context is found.\"\"\"\n",
        "    fallback_message = (\n",
        "        \"I don't have sufficient relevant information in my knowledge base \"\n",
        "        \"to answer your question about AI usage. Could you please rephrase \"\n",
        "        \"your question or ask about a different aspect of how people use AI?\"\n",
        "    )\n",
        "    return {\"response\": fallback_message}\n",
        "\n",
        "def conditional_router(state: EnhancedState) -> Literal[\"generate\", \"fallback_response\"]:\n",
        "    \"\"\"Route based on context sufficiency.\"\"\"\n",
        "    return \"generate\" if state[\"has_sufficient_context\"] else \"fallback_response\"\n",
        "\n",
        "# Modified graph construction\n",
        "enhanced_graph_builder = StateGraph(EnhancedState)\n",
        "enhanced_graph_builder.add_node(\"retrieve\", retrieve)\n",
        "enhanced_graph_builder.add_node(\"validate_context\", validate_context)\n",
        "enhanced_graph_builder.add_node(\"generate\", generate)\n",
        "enhanced_graph_builder.add_node(\"fallback_response\", fallback_response)\n",
        "\n",
        "# Add edges\n",
        "enhanced_graph_builder.add_edge(START, \"retrieve\")\n",
        "enhanced_graph_builder.add_edge(\"retrieve\", \"validate_context\")\n",
        "enhanced_graph_builder.add_conditional_edges(\n",
        "    \"validate_context\",\n",
        "    conditional_router,\n",
        "    {\n",
        "        \"generate\": \"generate\",\n",
        "        \"fallback_response\": \"fallback_response\"\n",
        "    }\n",
        ")\n",
        "enhanced_graph_builder.add_edge(\"generate\", END)\n",
        "enhanced_graph_builder.add_edge(\"fallback_response\", END)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### Implementation:\n",
        "\n",
        "from typing import Literal\n",
        "from langgraph.graph import START, END, StateGraph\n",
        "from typing_extensions import TypedDict\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "class EnhancedState(TypedDict):\n",
        "    question: str\n",
        "    context: list[Document]\n",
        "    response: str\n",
        "    context_quality: float  # New field for context relevance score\n",
        "    has_sufficient_context: bool  # New field for routing decision\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_context(state: EnhancedState) -> EnhancedState:\n",
        "    \"\"\"Validate if retrieved context is sufficient and relevant.\"\"\"\n",
        "    context = state[\"context\"]\n",
        "    question = state[\"question\"]\n",
        "    \n",
        "    if not context or len(context) == 0:\n",
        "        return {\n",
        "            \"context_quality\": 0.0,\n",
        "            \"has_sufficient_context\": False\n",
        "        }\n",
        "    \n",
        "    # Calculate relevance score using embedding similarity\n",
        "    question_embedding = embedding_model.embed_query(question)\n",
        "    context_embeddings = [embedding_model.embed_query(doc.page_content) for doc in context]\n",
        "    \n",
        "    # Calculate average cosine similarity\n",
        "    from numpy import dot\n",
        "    from numpy.linalg import norm\n",
        "    \n",
        "    similarities = []\n",
        "    for ctx_emb in context_embeddings:\n",
        "        similarity = dot(question_embedding, ctx_emb) / (norm(question_embedding) * norm(ctx_emb))\n",
        "        similarities.append(similarity)\n",
        "    \n",
        "    avg_similarity = sum(similarities) / len(similarities)\n",
        "    \n",
        "    # Set threshold for sufficient context (adjustable)\n",
        "    threshold = 0.3\n",
        "    \n",
        "    return {\n",
        "        \"context_quality\": avg_similarity,\n",
        "        \"has_sufficient_context\": avg_similarity >= threshold\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fallback_response(state: EnhancedState) -> EnhancedState:\n",
        "    \"\"\"Generate response when no sufficient context is found.\"\"\"\n",
        "    fallback_message = (\n",
        "        \"I don't have sufficient relevant information in my knowledge base \"\n",
        "        \"to answer your question about AI usage. Could you please rephrase \"\n",
        "        \"your question or ask about a different aspect of how people use AI?\"\n",
        "    )\n",
        "    return {\"response\": fallback_message}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "def conditional_router(state: EnhancedState) -> Literal[\"generate\", \"fallback_response\"]:\n",
        "    \"\"\"Route based on context sufficiency.\"\"\"\n",
        "    return \"generate\" if state[\"has_sufficient_context\"] else \"fallback_response\"\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x12f1b90a0>"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Modified graph construction\n",
        "enhanced_graph_builder = StateGraph(EnhancedState)\n",
        "enhanced_graph_builder.add_node(\"retrieve\", retrieve)\n",
        "enhanced_graph_builder.add_node(\"validate_context\", validate_context)\n",
        "enhanced_graph_builder.add_node(\"generate\", generate)\n",
        "enhanced_graph_builder.add_node(\"fallback_response\", fallback_response)\n",
        "\n",
        "# Add edges\n",
        "enhanced_graph_builder.add_edge(START, \"retrieve\")\n",
        "enhanced_graph_builder.add_edge(\"retrieve\", \"validate_context\")\n",
        "enhanced_graph_builder.add_conditional_edges(\n",
        "    \"validate_context\",\n",
        "    conditional_router,\n",
        "    {\n",
        "        \"generate\": \"generate\",\n",
        "        \"fallback_response\": \"fallback_response\"\n",
        "    }\n",
        ")\n",
        "enhanced_graph_builder.add_edge(\"generate\", END)\n",
        "enhanced_graph_builder.add_edge(\"fallback_response\", END)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "enhanced_graph = enhanced_graph_builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                  +-----------+               \n",
            "                  | __start__ |               \n",
            "                  +-----------+               \n",
            "                        *                     \n",
            "                        *                     \n",
            "                        *                     \n",
            "                  +----------+                \n",
            "                  | retrieve |                \n",
            "                  +----------+                \n",
            "                        *                     \n",
            "                        *                     \n",
            "                        *                     \n",
            "              +------------------+            \n",
            "              | validate_context |            \n",
            "              +------------------+            \n",
            "                 ..            ..             \n",
            "               ..                ..           \n",
            "             ..                    ..         \n",
            "+-------------------+           +----------+  \n",
            "| fallback_response |           | generate |  \n",
            "+-------------------+           +----------+  \n",
            "                 **            **             \n",
            "                   **        **               \n",
            "                     **    **                 \n",
            "                   +---------+                \n",
            "                   | __end__ |                \n",
            "                   +---------+                \n"
          ]
        }
      ],
      "source": [
        "# 2. Then visualize it using one of these methods:\n",
        "\n",
        "# Method 1: ASCII representation (works in any environment)\n",
        "enhanced_graph.get_graph().print_ascii()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Graph nodes: {'__start__': Node(id='__start__', name='__start__', data=RunnableCallable(tags=None, recurse=True, explode_args=False, func_accepts={}), metadata=None), 'retrieve': Node(id='retrieve', name='retrieve', data=retrieve(tags=None, recurse=True, explode_args=False, func_accepts={}), metadata=None), 'validate_context': Node(id='validate_context', name='validate_context', data=validate_context(tags=None, recurse=True, explode_args=False, func_accepts={}), metadata=None), 'generate': Node(id='generate', name='generate', data=generate(tags=None, recurse=True, explode_args=False, func_accepts={}), metadata=None), 'fallback_response': Node(id='fallback_response', name='fallback_response', data=fallback_response(tags=None, recurse=True, explode_args=False, func_accepts={}), metadata=None), '__end__': Node(id='__end__', name='__end__', data=None, metadata=None)}\n",
            "Graph edges: [Edge(source='__start__', target='retrieve', data=None, conditional=False), Edge(source='retrieve', target='validate_context', data=None, conditional=False), Edge(source='validate_context', target='fallback_response', data=None, conditional=True), Edge(source='validate_context', target='generate', data=None, conditional=True), Edge(source='fallback_response', target='__end__', data=None, conditional=False), Edge(source='generate', target='__end__', data=None, conditional=False)]\n"
          ]
        }
      ],
      "source": [
        "# Method 2: If you want to see the graph structure\n",
        "print(\"Graph nodes:\", enhanced_graph.get_graph().nodes)\n",
        "print(\"Graph edges:\", enhanced_graph.get_graph().edges)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Handling Response Fact-Checking\n",
        "\n",
        "**Problem**: Generated responses may contain inaccuracies or hallucinations even with good context.\n",
        "\n",
        "**Solution**: Add a fact-checking node with verification and correction capabilities.\n",
        "\n",
        "#### Extended Graph Structure:\n",
        "```\n",
        "START → retrieve → validate_context → generate → fact_check → [conditional routing]\n",
        "                                                      ↓\n",
        "                                              [verified] → END\n",
        "                                                      ↓\n",
        "                                              [needs_correction] → correct_response → END\n",
        "```\n",
        "\n",
        "#### Implementation:\n",
        "\n",
        "```python\n",
        "class FactCheckedState(TypedDict):\n",
        "    question: str\n",
        "    context: list[Document]\n",
        "    response: str\n",
        "    context_quality: float\n",
        "    has_sufficient_context: bool\n",
        "    fact_check_score: float  # New field\n",
        "    needs_correction: bool   # New field\n",
        "    corrected_response: str  # New field\n",
        "\n",
        "def fact_check(state: FactCheckedState) -> FactCheckedState:\n",
        "    \"\"\"Fact-check the generated response against the context.\"\"\"\n",
        "    response = state[\"response\"]\n",
        "    context = state[\"context\"]\n",
        "    \n",
        "    # Create a fact-checking prompt\n",
        "    fact_check_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"human\", \"\"\"\n",
        "        CONTEXT:\n",
        "        {context}\n",
        "        \n",
        "        GENERATED_RESPONSE:\n",
        "        {response}\n",
        "        \n",
        "        Please analyze if the generated response is factually accurate based on the provided context.\n",
        "        Rate the accuracy on a scale of 0.0 to 1.0 where:\n",
        "        - 1.0 = Completely accurate and supported by context\n",
        "        - 0.7-0.9 = Mostly accurate with minor issues\n",
        "        - 0.4-0.6 = Partially accurate but has some errors\n",
        "        - 0.0-0.3 = Largely inaccurate or unsupported\n",
        "        \n",
        "        Respond with just the numerical score (e.g., 0.8).\n",
        "        \"\"\")\n",
        "    ])\n",
        "    \n",
        "    fact_check_chain = fact_check_prompt | ollama_chat_model | StrOutputParser()\n",
        "    \n",
        "    context_text = \"\\n\".join([doc.page_content for doc in context])\n",
        "    score_response = fact_check_chain.invoke({\n",
        "        \"context\": context_text,\n",
        "        \"response\": response\n",
        "    })\n",
        "    \n",
        "    try:\n",
        "        fact_check_score = float(score_response.strip())\n",
        "    except ValueError:\n",
        "        fact_check_score = 0.5  # Default to moderate confidence\n",
        "    \n",
        "    # Set threshold for acceptable accuracy\n",
        "    accuracy_threshold = 0.7\n",
        "    \n",
        "    return {\n",
        "        \"fact_check_score\": fact_check_score,\n",
        "        \"needs_correction\": fact_check_score < accuracy_threshold\n",
        "    }\n",
        "\n",
        "def correct_response(state: FactCheckedState) -> FactCheckedState:\n",
        "    \"\"\"Generate a corrected response with explicit fact-checking.\"\"\"\n",
        "    context = state[\"context\"]\n",
        "    question = state[\"question\"]\n",
        "    original_response = state[\"response\"]\n",
        "    \n",
        "    correction_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"human\", \"\"\"\n",
        "        CONTEXT:\n",
        "        {context}\n",
        "        \n",
        "        ORIGINAL_QUESTION:\n",
        "        {question}\n",
        "        \n",
        "        ORIGINAL_RESPONSE (which may contain inaccuracies):\n",
        "        {original_response}\n",
        "        \n",
        "        Please provide a corrected and more accurate response to the original question.\n",
        "        Base your answer STRICTLY on the provided context. If any information cannot be\n",
        "        verified from the context, explicitly state \"This information is not available\n",
        "        in the provided context.\"\n",
        "        \n",
        "        Be conservative and only make claims that are directly supported by the context.\n",
        "        \"\"\")\n",
        "    ])\n",
        "    \n",
        "    correction_chain = correction_prompt | ollama_chat_model | StrOutputParser()\n",
        "    \n",
        "    context_text = \"\\n\".join([doc.page_content for doc in context])\n",
        "    corrected_response = correction_chain.invoke({\n",
        "        \"context\": context_text,\n",
        "        \"question\": question,\n",
        "        \"original_response\": original_response\n",
        "    })\n",
        "    \n",
        "    return {\"corrected_response\": corrected_response}\n",
        "\n",
        "def fact_check_router(state: FactCheckedState) -> Literal[\"finalize\", \"correct_response\"]:\n",
        "    \"\"\"Route based on fact-check results.\"\"\"\n",
        "    return \"correct_response\" if state[\"needs_correction\"] else \"finalize\"\n",
        "\n",
        "def finalize(state: FactCheckedState) -> FactCheckedState:\n",
        "    \"\"\"Finalize the response (use corrected if available, otherwise original).\"\"\"\n",
        "    if state.get(\"corrected_response\"):\n",
        "        final_response = state[\"corrected_response\"]\n",
        "    else:\n",
        "        final_response = state[\"response\"]\n",
        "    \n",
        "    return {\"response\": final_response}\n",
        "```\n",
        "\n",
        "## Complete Enhanced Graph Implementation\n",
        "\n",
        "```python\n",
        "# Complete enhanced graph with both edge case handlers\n",
        "complete_enhanced_graph = StateGraph(FactCheckedState)\n",
        "\n",
        "# Add all nodes\n",
        "complete_enhanced_graph.add_node(\"retrieve\", retrieve)\n",
        "complete_enhanced_graph.add_node(\"validate_context\", validate_context)\n",
        "complete_enhanced_graph.add_node(\"generate\", generate)\n",
        "complete_enhanced_graph.add_node(\"fallback_response\", fallback_response)\n",
        "complete_enhanced_graph.add_node(\"fact_check\", fact_check)\n",
        "complete_enhanced_graph.add_node(\"correct_response\", correct_response)\n",
        "complete_enhanced_graph.add_node(\"finalize\", finalize)\n",
        "\n",
        "# Add edges and conditional routing\n",
        "complete_enhanced_graph.add_edge(START, \"retrieve\")\n",
        "complete_enhanced_graph.add_edge(\"retrieve\", \"validate_context\")\n",
        "\n",
        "# Route based on context quality\n",
        "complete_enhanced_graph.add_conditional_edges(\n",
        "    \"validate_context\",\n",
        "    conditional_router,\n",
        "    {\n",
        "        \"generate\": \"generate\",\n",
        "        \"fallback_response\": \"fallback_response\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Fact-check generated responses\n",
        "complete_enhanced_graph.add_edge(\"generate\", \"fact_check\")\n",
        "\n",
        "# Route based on fact-check results\n",
        "complete_enhanced_graph.add_conditional_edges(\n",
        "    \"fact_check\",\n",
        "    fact_check_router,\n",
        "    {\n",
        "        \"finalize\": \"finalize\",\n",
        "        \"correct_response\": \"correct_response\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Finalize corrected responses\n",
        "complete_enhanced_graph.add_edge(\"correct_response\", \"finalize\")\n",
        "\n",
        "# End paths\n",
        "complete_enhanced_graph.add_edge(\"fallback_response\", END)\n",
        "complete_enhanced_graph.add_edge(\"finalize\", END)\n",
        "\n",
        "# Compile the enhanced graph\n",
        "enhanced_rag_graph = complete_enhanced_graph.compile()\n",
        "```\n",
        "\n",
        "## Additional Enhancements\n",
        "\n",
        "### 3. Query Clarification Node\n",
        "For ambiguous queries, add a clarification node:\n",
        "\n",
        "```python\n",
        "def clarify_query(state: FactCheckedState) -> FactCheckedState:\n",
        "    \"\"\"Detect and handle ambiguous queries.\"\"\"\n",
        "    # Implementation for query ambiguity detection\n",
        "    pass\n",
        "```\n",
        "\n",
        "### 4. Multi-Step Reasoning\n",
        "For complex questions requiring multiple retrieval steps:\n",
        "\n",
        "```python\n",
        "def decompose_query(state: FactCheckedState) -> FactCheckedState:\n",
        "    \"\"\"Break complex queries into sub-questions.\"\"\"\n",
        "    # Implementation for query decomposition\n",
        "    pass\n",
        "```\n",
        "\n",
        "### 5. Confidence Scoring\n",
        "Add confidence scores to responses:\n",
        "\n",
        "```python\n",
        "def add_confidence_score(state: FactCheckedState) -> FactCheckedState:\n",
        "    \"\"\"Add confidence scoring to responses.\"\"\"\n",
        "    # Implementation for confidence estimation\n",
        "    pass\n",
        "```\n",
        "\n",
        "## Benefits of This Approach\n",
        "\n",
        "1. **Robustness**: Handles edge cases gracefully\n",
        "2. **Transparency**: Users know when information is insufficient\n",
        "3. **Accuracy**: Fact-checking reduces hallucinations\n",
        "4. **Flexibility**: Easy to add more validation steps\n",
        "5. **Maintainability**: Clear separation of concerns in nodes\n",
        "6. **Observability**: Each step can be monitored and debugged\n",
        "\n",
        "This enhanced implementation transforms the simple linear RAG pipeline into a robust, production-ready system that can handle real-world edge cases while maintaining the clarity and modularity that LangGraph provides.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FactCheckedState(TypedDict):\n",
        "    question: str\n",
        "    context: list[Document]\n",
        "    response: str\n",
        "    context_quality: float\n",
        "    has_sufficient_context: bool\n",
        "    fact_check_score: float  # New field\n",
        "    needs_correction: bool   # New field\n",
        "    corrected_response: str  # New field\n",
        "\n",
        "def fact_check(state: FactCheckedState) -> FactCheckedState:\n",
        "    \"\"\"Fact-check the generated response against the context.\"\"\"\n",
        "    response = state[\"response\"]\n",
        "    context = state[\"context\"]\n",
        "    \n",
        "    # Create a fact-checking prompt\n",
        "    fact_check_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"human\", \"\"\"\n",
        "        CONTEXT:\n",
        "        {context}\n",
        "        \n",
        "        GENERATED_RESPONSE:\n",
        "        {response}\n",
        "        \n",
        "        Please analyze if the generated response is factually accurate based on the provided context.\n",
        "        Rate the accuracy on a scale of 0.0 to 1.0 where:\n",
        "        - 1.0 = Completely accurate and supported by context\n",
        "        - 0.7-0.9 = Mostly accurate with minor issues\n",
        "        - 0.4-0.6 = Partially accurate but has some errors\n",
        "        - 0.0-0.3 = Largely inaccurate or unsupported\n",
        "        \n",
        "        Respond with just the numerical score (e.g., 0.8).\n",
        "        \"\"\")\n",
        "    ])\n",
        "    \n",
        "    fact_check_chain = fact_check_prompt | ollama_chat_model | StrOutputParser()\n",
        "    \n",
        "    context_text = \"\\n\".join([doc.page_content for doc in context])\n",
        "    score_response = fact_check_chain.invoke({\n",
        "        \"context\": context_text,\n",
        "        \"response\": response\n",
        "    })\n",
        "    \n",
        "    try:\n",
        "        fact_check_score = float(score_response.strip())\n",
        "    except ValueError:\n",
        "        fact_check_score = 0.5  # Default to moderate confidence\n",
        "    \n",
        "    # Set threshold for acceptable accuracy\n",
        "    accuracy_threshold = 0.7\n",
        "    \n",
        "    return {\n",
        "        \"fact_check_score\": fact_check_score,\n",
        "        \"needs_correction\": fact_check_score < accuracy_threshold\n",
        "    }\n",
        "\n",
        "def correct_response(state: FactCheckedState) -> FactCheckedState:\n",
        "    \"\"\"Generate a corrected response with explicit fact-checking.\"\"\"\n",
        "    context = state[\"context\"]\n",
        "    question = state[\"question\"]\n",
        "    original_response = state[\"response\"]\n",
        "    \n",
        "    correction_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"human\", \"\"\"\n",
        "        CONTEXT:\n",
        "        {context}\n",
        "        \n",
        "        ORIGINAL_QUESTION:\n",
        "        {question}\n",
        "        \n",
        "        ORIGINAL_RESPONSE (which may contain inaccuracies):\n",
        "        {original_response}\n",
        "        \n",
        "        Please provide a corrected and more accurate response to the original question.\n",
        "        Base your answer STRICTLY on the provided context. If any information cannot be\n",
        "        verified from the context, explicitly state \"This information is not available\n",
        "        in the provided context.\"\n",
        "        \n",
        "        Be conservative and only make claims that are directly supported by the context.\n",
        "        \"\"\")\n",
        "    ])\n",
        "    \n",
        "    correction_chain = correction_prompt | ollama_chat_model | StrOutputParser()\n",
        "    \n",
        "    context_text = \"\\n\".join([doc.page_content for doc in context])\n",
        "    corrected_response = correction_chain.invoke({\n",
        "        \"context\": context_text,\n",
        "        \"question\": question,\n",
        "        \"original_response\": original_response\n",
        "    })\n",
        "    \n",
        "    return {\"corrected_response\": corrected_response}\n",
        "\n",
        "def fact_check_router(state: FactCheckedState) -> Literal[\"finalize\", \"correct_response\"]:\n",
        "    \"\"\"Route based on fact-check results.\"\"\"\n",
        "    return \"correct_response\" if state[\"needs_correction\"] else \"finalize\"\n",
        "\n",
        "def finalize(state: FactCheckedState) -> FactCheckedState:\n",
        "    \"\"\"Finalize the response (use corrected if available, otherwise original).\"\"\"\n",
        "    if state.get(\"corrected_response\"):\n",
        "        final_response = state[\"corrected_response\"]\n",
        "    else:\n",
        "        final_response = state[\"response\"]\n",
        "    \n",
        "    return {\"response\": final_response}\n",
        "\n",
        "\n",
        "## Complete Enhanced Graph Implementation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete enhanced graph with both edge case handlers\n",
        "complete_enhanced_graph = StateGraph(FactCheckedState)\n",
        "\n",
        "# Add all nodes\n",
        "complete_enhanced_graph.add_node(\"retrieve\", retrieve)\n",
        "complete_enhanced_graph.add_node(\"validate_context\", validate_context)\n",
        "complete_enhanced_graph.add_node(\"generate\", generate)\n",
        "complete_enhanced_graph.add_node(\"fallback_response\", fallback_response)\n",
        "complete_enhanced_graph.add_node(\"fact_check\", fact_check)\n",
        "complete_enhanced_graph.add_node(\"correct_response\", correct_response)\n",
        "complete_enhanced_graph.add_node(\"finalize\", finalize)\n",
        "\n",
        "# Add edges and conditional routing\n",
        "complete_enhanced_graph.add_edge(START, \"retrieve\")\n",
        "complete_enhanced_graph.add_edge(\"retrieve\", \"validate_context\")\n",
        "\n",
        "# Route based on context quality\n",
        "complete_enhanced_graph.add_conditional_edges(\n",
        "    \"validate_context\",\n",
        "    conditional_router,\n",
        "    {\n",
        "        \"generate\": \"generate\",\n",
        "        \"fallback_response\": \"fallback_response\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Fact-check generated responses\n",
        "complete_enhanced_graph.add_edge(\"generate\", \"fact_check\")\n",
        "\n",
        "# Route based on fact-check results\n",
        "complete_enhanced_graph.add_conditional_edges(\n",
        "    \"fact_check\",\n",
        "    fact_check_router,\n",
        "    {\n",
        "        \"finalize\": \"finalize\",\n",
        "        \"correct_response\": \"correct_response\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Finalize corrected responses\n",
        "complete_enhanced_graph.add_edge(\"correct_response\", \"finalize\")\n",
        "\n",
        "# End paths\n",
        "complete_enhanced_graph.add_edge(\"fallback_response\", END)\n",
        "complete_enhanced_graph.add_edge(\"finalize\", END)\n",
        "\n",
        "# Compile the enhanced graph\n",
        "enhanced_rag_graph = complete_enhanced_graph.compile()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                               +-----------+                                \n",
            "                               | __start__ |                                \n",
            "                               +-----------+                                \n",
            "                                      *                                     \n",
            "                                      *                                     \n",
            "                                      *                                     \n",
            "                                +----------+                                \n",
            "                                | retrieve |                                \n",
            "                                +----------+                                \n",
            "                                      *                                     \n",
            "                                      *                                     \n",
            "                                      *                                     \n",
            "                            +------------------+                            \n",
            "                            | validate_context |                            \n",
            "                            +------------------+.                           \n",
            "                              ..                 ....                       \n",
            "                            ..                       ....                   \n",
            "                          ..                             ....               \n",
            "                  +----------+                               ...            \n",
            "                  | generate |                                 .            \n",
            "                  +----------+                                 .            \n",
            "                        *                                      .            \n",
            "                        *                                      .            \n",
            "                        *                                      .            \n",
            "                 +------------+                                .            \n",
            "                 | fact_check |                                .            \n",
            "                 +------------+                                .            \n",
            "                ..            ...                              .            \n",
            "              ..                 ..                            .            \n",
            "            ..                     ..                          .            \n",
            "+------------------+                 ..                        .            \n",
            "| correct_response |               ..                          .            \n",
            "+------------------+             ..                            .            \n",
            "                **            ...                              .            \n",
            "                  **        ..                                 .            \n",
            "                    **    ..                                   .            \n",
            "                  +----------+                       +-------------------+  \n",
            "                  | finalize |                       | fallback_response |  \n",
            "                  +----------+                       +-------------------+  \n",
            "                              **                 ****                       \n",
            "                                **           ****                           \n",
            "                                  **      ***                               \n",
            "                                +---------+                                 \n",
            "                                | __end__ |                                 \n",
            "                                +---------+                                 \n"
          ]
        }
      ],
      "source": [
        "# Method 1: ASCII representation (works in any environment)\n",
        "enhanced_rag_graph.get_graph().print_ascii()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Additional Enhancements\n",
        "\n",
        "### 3. Query Clarification Node\n",
        "#For ambiguous queries, add a clarification node:\n",
        "\n",
        "def clarify_query(state: FactCheckedState) -> FactCheckedState:\n",
        "    \"\"\"Detect and handle ambiguous queries.\"\"\"\n",
        "    # Implementation for query ambiguity detection\n",
        "    pass\n",
        "\n",
        "\n",
        "### 4. Multi-Step Reasoning\n",
        "#For complex questions requiring multiple retrieval steps:\n",
        "\n",
        "\n",
        "def decompose_query(state: FactCheckedState) -> FactCheckedState:\n",
        "    \"\"\"Break complex queries into sub-questions.\"\"\"\n",
        "    # Implementation for query decomposition\n",
        "    pass\n",
        "\n",
        "\n",
        "### 5. Confidence Scoring\n",
        "#Add confidence scores to responses:\n",
        "\n",
        "\n",
        "def add_confidence_score(state: FactCheckedState) -> FactCheckedState:\n",
        "    \"\"\"Add confidence scoring to responses.\"\"\"\n",
        "    # Implementation for confidence estimation\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
